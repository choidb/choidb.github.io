# 랜덤포레스트(Random Forests)

**목차**  
[1. 랜덤포레스트](#1-랜덤포레스트)  
[2. 범주형 자료의 순서형(ordinal) 인코딩](#2-범주형-자료의-순서형ordinal-인코딩)  
[3. 특성중요도](#3-특성중요도)  


---
* Contents
1. 랜덤포레스트 모델
2. 순서형인코딩(ordinal encoding)
3. 범주형 변수의 인코딩 방법의 영향
4. Bagging
5. Out-Of-Bag(OOB) sample

* 라이브러리
  * category_encoders
  * ipywidgets
  * matplotlib
  * pandas
  * scikit-learn

---

## 1. 랜덤포레스트
[랜덤포레스트](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees) 참고
* 랜덤포레스트는 결정트리를 기본모델로 사용하는 앙상블 방법
  * 앙상블 방법은 한 종류의 데이터로 여러 머신러닝 학습모델(weak base learner, 기본모델)을 만들어 그 모델들의 예측결과를 다수결이나 평균을 내어 예측하는 방법
  * 이론적으로 기본모델 몇가지 조건을 충족하는 여러 종류의 모델을 사용 가능
  * 결정트리들은 독립적으로 만들어지며 각각 랜덤으로 예측하는 성능보다 좋을 경우 랜덤포레스트는 결정트리보다 성능이 좋음
* 랜덤포레스트 기본모델
  * 배깅(Bagging, Bootstrap Aggregating)
    * 부트스트랩(Bootstrap) 샘플링
      * 앙상블에 사용하는 작은 모델들은 부트스트래핑(bootstraping)이라는 샘플링과정으로 얻은 부트스트랩세트를 사용해 학습을 한다.
      * 원본 데이터에서 샘플링을 하는데 복원추출을 한다
        * 복원추출은 샘플을 뽑아 값을 기록하고 제자리에 돌려놓는 것
        * 복원추출이기 때문에 부트스트랩세트에는 같은 샘플이 반복될 수 있음
      * 부트스트랩세트의 크기가 n이라 할 때 한 번의 추출과정에서 어떤 한 샘플이 추출 되지 않을 확률
        * $\displaystyle \frac {n-1}{n}$
      * n회 복원추출을 진행했을 때 그 샘플이 추출되지 않았을 확률
        * $\displaystyle \left({\frac {n-1}{n}}\right)^{n}$
      * n을 무한히 크게 했을 때의 식
        * $\displaystyle \lim _{{n\to \infty }}\left({1 - \frac {1}{n}}\right)^{n} = e^{-1} = 0.368$
        * 참고: $\displaystyle e =  \lim _{{n\to \infty }}\left(1+{\frac  {1}{n}}\right)^{n}$
      * 데이터가 충분히 크다고 가정했을 때 한 부트스트랩세트는 **표본의 63.2%** 에 해당하는 샘플을 가짐
      * 여기서 추출되지 않는 36.8%의 샘플이 Out-Of-Bag 샘플이다.
    * 기본모델 합치는 과정
      * Aggregation 이라 함
        * 회귀문제일 경우 기본모델 결과들의 평균으로 결과를 내고
        * 분류문제일 경우 다수결로 가장 많은 모델들이 선택한 범주로 예측함
    * 랜덤포레스트는 기본모델들의 트리를 만들 때 무작위로 선택한 특성세트를 사용
    * 기본 모델 트리를 만드는 방법과 일반 결정트리 알고리즘의 차이
      * 결정트리에서 분할을 위한 특성을 선택할 때, 모든 특성(n개)을 고려하여 최적의 특성을 고르고 분할
      * 랜덤포레스트에서는 **특성 n개 중 일부분 k개의 특성을 선택(sampling)** 하고 이 k개에서 최적의 특성을 찾아내어 분할합니다. 이때 k개는 일반적으로 $log_2 n$ 를 사용

## 2. 범주형 자료의 순서형(ordinal) 인코딩
* 순서형 인코딩은 범주에 숫자를 맵핑함
* 트리구조 학습에서는 원핫인코딩을 사용하면 문제발생
  * 트리구조에서는 중요한 특성이 상위노드에서 먼저 분할이 일어남
  * 주 종류가 많은(high cardinality) 특성은 원핫인코딩으로 인해 상위노드에서 선택될 기회가 적어짐
  * 원핫인코딩 영향을 안 받는 수치형 특성이 상위노드를 차지할 기회가 높아지고 전체적인 성능 저하가 생길 수 있음.

## 3. 특성중요도
* 랜덤포레스트에서는 학습 후에 특성들의 중요도 정보(Gini importance)를 기본으로 제공합니다. 중요도는 노드들의 지니불순도(Gini impurity)를 가지고 계산하는데 노드가 중요할 수록 불순도가 크게 감소한다는 사실을 이용함
* 노드는 한 특성의 값을 기준으로 분리가 되기 때문에 불순도를 크게 감소하는데 많이 사용된 특성이 중요도가 올라갈 것임.
* 순서형인코딩은 주의해야 할 것이 있습니다. 범주들을 순서가 있는 숫자형으로 바꾸면 원래 그 범주에 없던 순서정보가 생김
* 정확한 범주의 순위를 알고 있다면 mapping 파라미터를 사용해 지정해줄 수 있음
* [OrdinalEncoder](https://contrib.scikit-learn.org/category_encoders/ordinal.html)

* 트리 앙상블 모델이 결정트리모델보다 상대적으로 과적합을 피할 수 있는 이유
  * 결정트리는 데이터 일부에 과적합하는 경향이 있습니다. 그래서 다르게 샘플링된 데이터로 과적합된 트리를 많이 만들고 그 결과를 평균내 사용하는 모델이 랜덤 포레스트 입니다. 이렇게 하면 과적합이 줄고 성능이 유지 된다는 것이 알려져 있습니다. 랜텀포레스트에서 트리를 랜덤하게 만드는 방법은 두 가지 입니다.
    * 랜덤포레스트에서 학습되는 트리들은 배깅을 통해 만들어짐. 이때 각 기본트리에 사용되는 데이터가 랜덤으로 선택됨
    * 각각 트리는 무작위로 선택된 특성들을 가지고 분기를 수행함