---
title: '선형대수'
use_math: true
categories:
  - data
---

**목차**  
[1. Variance(분산)](#1-variance)  
[2. 표준편차](#2-standard-deviation표준편차)  
[3. 공분산](#3-covariance공분산)  
[4. Variance covariance matrix](#4-variance-covariance-matrix)  
[5. Crrelation coefficient](#5-correlation-coefficient)  
[6. Orthogonality](#6-orthogonality)  
[7. 단위벡터](#7-단위벡터--unit-vectors)  
[8. span](#8-span)  
[9. basis](#9-basis)  
[10. rank](#10-rank)  


---
* Contents
  1. 공분산, 상관계수
  2. 벡터의 직교
  3. 단위 벡터
  4. span, basis, rank
  5. Gaussian elemination
  6. linear projection

---

## 1. Variance(분산)

* 분산은, 데이터가 얼마나 퍼져있는지를 측정하는 방법
  * 각 값들의 평균으로부터 차이의 제곱 평균


$\overline{X}$ 는 평균, $N$ 은 관측의 수 입니다. (샘플의 수)

$v$ 혹은 분산은 일반적으로 소문자 v로 표기되며 필요에 따라 $\sigma^{2}$로 표기되기도 합니다.


$v = \frac{\sum{(X_{i} - \overline{X})^{2}} }{N}$

* 평균을 계산하는 방법은, 전부 더한 다음 수로 나누는 것이다.

* Variance 쉽게 계산하기
  * 분산을 계산하는 방법이 모집단이냐 혹은 샘플이냐에 따라서 달라지기 때문에 주의할 것.
```python
print(variance_df.v1.var(ddof = 1))
print(variance_df.v2.var(ddof = 1))
# v1, v2: 데이터 프레임에서의 열 이름
```

 **모집단의 분산** $\sigma^{2}$ 는 모집단의 **PARAMETER** (aspect, property, attribute, etc)이며,

**샘플의 분산** $s^{2}$ 는 샘플의 **STATISTIC** (estimated attribute)입니다.

복잡한 차이가 있기는 하지만, 샘플 분산 $s^{2}$ 는 모집단 분산 $\sigma^{2}$의 추정치입니다.

일반적으로, **샘플**의 분산을 계산 할때는 $N-1$로 나누어야 합니다. 

앞서 우리가 데이터를 통해 계산했던 방식은 **모집단의 분산**이다.

그렇기 때문에 만약 우리가 자유도를 0 으로 설정하는 경우, 동일한 값을 얻을 수 있습니다.

## 2. Standard Deviation(표준편차)

* 표준편차는 분산의 값에 $\sqrt()$를 씌운 것.

* 분산을 구하는 과정에서 제곱값을 더하게 되는데 평균에 비해 스케일이 커지게 되므로 스케일을 낮추기 위한 방법이다.

## 3. Covariance(공분산)
* 1개의 변수 값이 변화할 때 다른 변수가 어떠한 연관성을 나타내며 변하는지를 측정
* 2개의 변수를 사용하기 때문에 그래프로 보는 것이 이해하는데 도움을 줌.

<img src="https://github.com/choidb/choidb.github.io/blob/master/_posts/images1/2022-06-22-17-44-17.png?raw=true" width="200" height="400"/>  
출처: https://ko.wikipedia.org/wiki/%EA%B3%B5%EB%B6%84%EC%82%B0

* 첫번째 그래프의 경우, y의 값이 높을때 x 의 값은 낮습니다. 이는 음의 (negative) 공분산 값을 갖는다 라고 표현합니다.

* 두번째 그래프에서는 양 변수의 높고 낮음에 대하여 관련성을 알 수 없습니다. 그러므로 이러한 형태는 0에 가까운 공분산 값을 갖습니다.

* 마지막 그래프에서, y 값이 낮을때 x의 값도 마찬가지로 낮으며, 높을때는 같이 높아진다는 것을 확인 할 수 있습니다. 이 경우 양 변수간의 공분산 값은 양의 (positive)값을 기대 할 수 있습니다.


* 큰 값의 공분산은 두 변수간의 큰 연관성을 나타냄


## 4. Variance-covariance matrix

* 공분산을 계산하기 위한 방법
```python
df.cov()
```
* 대각선 부분은 공분산이 아닌, 분산을 표현

## 5. Correlation coefficient
* 공분산의 스케일을 조정방법
* 공분산을 두 변수의 표준편차로 각각 나눠주면 스케일을 조정할 수 있으며 상관계수라고 부름.
* 상관계수가 공분산에 비해서 더 좋은 지표로써 사용되는 이유
  * 공분산은 이론상 모든 값을 가질 수 있지만, 상관계수는 -1 ~ 1 사이로 정해져 비교하기가 쉽습니다.
  * 공분산은 항상 스케일, 단위를 포함하고 있지만, 상관계수는 이에 영향을 받지 않습니다.
  * 상관계수는 데이터의 평균 혹은 분산의 크기에 영향을 받지 않습니다.
    * 상관계수
      * $cor(X,Y) = r = \frac{cov(X,Y)}{\sigma_{X}\sigma_{Y}}$

## 6. Orthogonality
* 벡터 혹은 매트릭스가 서로 수직으로 있는 상태를 나타냄
* 벡터의 orthogonality 확인
  * 임의의 두 벡터의 내적값이 0이라면 서로 수직으로 배치되어 있다.

## 7. 단위벡터 ( Unit Vectors )
* 선형대수에서, 단위 벡터란 "단위 길이(1)"를 갖는 모든 벡터

$v$ = [1, 2, 2]

$||v||$ = $\sqrt{1^2 + 2^2 + 2^2}$ = 3

$\hat{v}$ = 1 / $||v||$ $\cdot$ $v$ 

= $1 \over 3$ $\cdot$ [1, 2, 2] = [$1 \over 3$, $2 \over 3$, $2 \over 3$]

$||\hat{v}||$ = 1

---

$\mathbb{R}$ unit vector: $\hat{i} = \begin{bmatrix} 1 \end{bmatrix}$

$\mathbb{R}^2$ unit vectors: $\hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, $\hat{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$

$\mathbb{R}^3$ unit vectors: $\hat{i} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$, $\hat{j} = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$,  $\hat{k} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$

* 벡터를 단위 벡터의 조합으로 표기
  * 모든 벡터 ( 혹은 매트릭스 )는 단위 벡터의 선형 조합으로 표기됨

## 8. Span
* Span 이란, 주어진 두 벡터의 (합이나 차와 같은) 조합으로 만들 수 있는 모든 가능한 벡터의 집합이다.

* 선형 관계의 벡터 (Linearly Dependent Vector)
  * 만약 두 벡터가 같은 선상에 있는 경우, 이 벡터들은 선형 관계에 있다고 표현합니다. 즉, 이 두 벡터들은 조합을 통해서 선 외부의 새로운 벡터를 생성 할 수 없습니다.
  * 이러한 벡터의 span은, 평면 공간이 아닌, 벡터가 이미 올려져 있는 선으로 제한된다.
* 선형 관계가 없는 벡터 (Linearly Independent Vectors)
  * 반대로 같은 선상에 있지 않은 벡터들은 선형적으로 독립되어 있다고 표현하며, 주어진 공간 (2개의 벡터의 경우  R2  평면)의 모든 벡터를 조합을 통해 만들어 낼 수 있다.

## 9. Basis
* 벡터 공간  V 의 basis 는,  V  라는 공간을 채울 수 있는 선형 관계에 있지 않은 벡터들의 모음( span 의 역개념 )
* Orthogonal Basis
  * Basis 에 추가로 Orthogonal 한 조건이 붙는, 즉 주어진 공간을 채울 수 있는 서로 수직인 벡터들
* Orthonormal Basis
  * Orthogonal Basis에 추가로 Normalized 조건이 붙은 것으로, 길이가 서로 1인 벡터들
* Gram-Schmidt 프로세스
  * 주어진 벡터들을 Orthonormal하게 수정하는 과정

## 10. Rank
* 매트릭스의 rank란, 매트릭스의 열을 이루고 있는 벡터들로 만들 수 있는 (span) 공간의 차원
* 매트릭스의 차원과는 다를 수도 있으며 그 이유는 행과 열을 이루고 있는 벡터들 가운데 서로 선형 관계가 있을 수도 있기 때문
* Rank 확인 방법
  * Gaussian Elimination(가우스 소거법)
    * 주어진 매트릭스를 "Row-Echelon form"으로 바꾸는 계산과정
      * "Row-Echelon form"이란, 각 행에 대해서 왼쪽에 1, 그 이후 부분은 0으로 이뤄진 형태
    * [[선형대수 (Linear Algebra)] 가우스 소거법 (Gaussian Elimination) 란? (Gauss-Jordan 소거법 포함)](https://m.blog.naver.com/sw4r/221942487071) 참고
