# 퍼셉트론(Perceptron)과 인공신경망(Artificial Neural Networks)

**목차**  
[1. 퍼셉트론](#1-퍼셉트론)  
[2. 인공신경망](#2-인공신경망)  
[3. 가중치 행렬](#3-가중치-행렬)  
[4. 활성화 함수의 공통점](#4-활성화-함수의-공통점)  

---
* Contents
1. 퍼셉트론
2. 신경망 구성(입력층,은닉층,출력층)
3. 가중치 행렬의 Shape, 신경망 구조
4. 활성화 함수
5. 표현학습

---

## 1. 퍼셉트론
* 신경망을 이루는 가장 기본 단위
* 다수의 신호를 입력으로 받아 하나의 신호를 출력하는 구조
* 퍼셉트론의 구성
  1. 가중치(Weight)-편향(bias) 연산
   * 퍼셉트론은 여러 개의 신호를 입력받아서 각각의 가중치와 곱해지고 그 결과를 더해줌(가중합(Weightes Sum))
   * $\sum(b + w_0x_0 + w_1x_1 + ... + w_nx_n)$
  2. 활성화 함수(Activation function)
   * 활성화 함수(Step, Sigmoid, ReLU, Softmax)
    1. 계단 함수(Step function)
        * 입력값이 임계값(여기서는 0)을 넘기면 1을, 그렇지 않으면 0을 출력하는 함수
    2. 시그모이드 함수(Sigmoid function)
        * 계단 함수에서 임계값 지점에서 미분이 불가능하고, 나머지 지점에서는 미분값이 0이 나오는 부분을 보완함.
        * 모든 지점에서 미분 가능하며, 미분값도 0이 아니다.
    3. ReLU 함수(ReLU function)
        * 시그모이드 함수를 중복하여 사용하게 되면 기울기 소실(Vanishing Gradient) 문제가 발생을 해결
        * 양의 값이 입력되면 그 값을 그대로 출력하고 음의 값이 입력되면 0을 반환
        * $f(x) = \max(0, x)$
        * 음수 구간에서는 미분값이 0이지만, 양수 구간에서는 미분값이 1
        * 층이 깊어지더라도 1의 값이 계속 곱해지기 때문에 기울기 값이 과도하게 커지거나 작아지는 문제가 발생하지 않게 됩니다.
    4. 소프트맥스 함수(Softmax function)
        * 다중 분류(Multi-classification) 문제에 적용할 수 있도록 시그모이드 함수를 일반화한 활성화 함수
        * 가중합 값을 소프트맥스 함수에 통과시키면 모든 클래스의 값의 합이 1이 되는 확률값으로 변환

* 논리 게이트(Logic Gate)
  * 퍼셉트론의 가장 단순한 형태
  * AND GATE
    * 입력 신호가 모두 1(True)일 때 1(True)을 출력
  * NAND GATE
    * Not AND 의 줄임말로 AND GATE의 결과의 반대를 출력
  * OR GATE
    * 입력 신호 중 하나만 1(True)이라도 1(True)을 출력
  * XOR GATE
    * 배타적 논리합(Exclusive-OR)이라고도 불리는 GATE
    * 입력 신호가 다를 경우 1(True)을 출력

## 2. 인공신경망
* ANN(Artificial Neural Networks), 즉 인공 신경망은 실제 신경계를 모사하여 만들어진 계산 모델
* 뉴럴넷(Neural-Net)으로 부르기도 함
* 다층으로 쌓는 이유
  * AND, NAND, OR GATE 를 다층으로 활용하면 XOR GATE를 구할 수 있음
  * 퍼셉트론을 여러 개의 층으로 쌓아 구축한 신경망을 다층 퍼셉트론 신경망(Multi-Layer Perceptron, MLP)이라 함
* 신경망의 층
  * 입력층(Input Layer)
    * 데이터셋이 입력되는 층
    * 데이터셋의 특성(Feature)에 따라 입력층 노드의 수가 결정
    * 입력층은 어떤 계산도 수행하지 않고 그냥 값들을 전달하기만 함
    * 신경망의 층수(깊이, depth)를 셀 때 입력층은 포함하지 않음
  * 은닉층(Hidden Layers)
    * 입력층으로부터 입력된 신호가 가중치, 편향과 연산되는 층
    * 입력층과 출력층 사이에 있는 층
    * 은닉층에서 일어나는 계산의 결과를 사용자가 볼 수 없기 때문에 '은닉(Hidden)층' 이라는 이름이 사용
    * 은닉층은 입력 데이터셋의 특성 수와 상관 없이 노드 수를 구성
    * 딥러닝(Deep Learning)이라고 하면 2개 이상의 은닉층을 가진 신경망을 말함
  * 출력층(Output Layer)
    * 가장 마지막에 위치한 층이며 은닉층 연산을 마친 값이 출력되는 층
    * 우리가 풀어야 할 문제 종류에 따라서 출력층을 잘 설계하는 것이 중요
    * 이진 분류(Binary Classification) : 활성화 함수로는 시그모이드(Sigmoid) 함수를 사용하며 출력층의 노드 수는 1로 설정합니다. 출력되는 값이 0과 1 사이의 확률값이 되도록 합니다.
    * 다중 분류(Multi-class Classification) : 활성화 함수로는 소프트맥스(Softmax) 함수를 사용하며 출력층의 노드 수는 레이블의 클래스(Class) 수와 동일하게 설정합니다.
    * 회귀(Regression) : 일반적으로는 활성화 함수를 지정해주지 않으며 출력층의 노드 수는 출력값의 특성(Feature) 수와 동일하게 설정합니다.
* 딥러닝 프레임워크(Framework)인 텐서플로우(Tensorflow)와 그 상위 API인 케라스(Keras)를 사용

## 3. 가중치 행렬
* 신경망에서 실제로 학습되는 부분은 각 가중치
* 입력층에 3개의 노드, 은닉층에 4개의 노드가 있었으면 두 층 사이에는 12개의 가중치가 존재
* 12개의 가중치가 연산되는 과정을 컴퓨터에서 잘 연산하기 위해서는 행렬의 형태로 만들어야 함
* 행렬의 shape
  *  가중치 행렬을 W, 입력 벡터를 x 라고 하면 연산의 결과로 출력되는 벡터는 y
  *  y = W x
  *  입력 벡터의 형태에 따라 가중치 행렬의 Shape 이 결정되어야 함
* Keras로 구축한 신경망에서 가중치 행렬의 Shape
  1. 입력층-은닉층 사이의 가중치 행렬
    * input_shape=100 으로부터 입력되는 데이터의 특성이 100개
    * 입력층의 노드도 100개 생성
    * 입력층과 연결되는 은닉층의 노드 개수가 10개 이므로 두 층 사이에 생성되는 가중치 행렬의 Shape 은 (100, 10)
  2. 은닉층-출력층 사이의 가중치 행렬
    * 은닉층의 노드가 10개 이며, 출력층의 노드 개수가 1개 이므로 두 층 사이에 가중치 행렬의 Shape 은 (10, 1)

## 4. 활성화 함수의 공통점
* 모든 활성화 함수의 공통점은 **비선형**(**Non-linear**)이라는 점이다.
* 선형이면 안되는 이유
  * 층을 아무리 깊게 쌓아도 여러 층을 쌓는 이점을 살리지 못하게 된다.
  * 여러 층을 쌓을 때의 장점을 살리기 위해 비선형 함수를 사용함.