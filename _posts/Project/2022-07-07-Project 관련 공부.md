# PostgreSQL & RDKit Cartridge & Docker

* RDKit은 PostgreSQL용 Database Cartridge를 제공한다.

## Docker 기반 PostgreSQL 컨테이너를 띄우고 해당 환경에서 ChemBL 데이터베이스를 생성하는 방법

1. Docker image 다운 및 컨테이너 실행
Git Bash에서 설정함

```
$ docker run --name dockerpostgres -p 5432:5432 -e POSTGRES_PASSWORD=(postgreSQL비밀번호) -d mcs07/postgres-rdkit

or

$ docker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -d postgres
아래 코드는 rdkit 없는건가
```
    > --name: 컨테이너 이름
    > dockerpostgres: NAMES(정할 수 있음)
    > -d: 백그라운드로 컨테이너 실행
    > mcs07/postgres-rdkit: 불러올 docker image 이름


```
$ docker ps
```
> Docker의 정보를 볼수 있음(CONTAINER ID, IMAGE, COMMAND, NAMES ...)

2. 컨테이너 접속 및 ChEMBL 데이터 다운로드

```
$ docker exec -it dockerpostgres /bin/bash
```
> 도커의 PostgreSQL 컨테이너로 접속하게 된다.

```
$ wget ftp://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/latest/chembl_30_postgresql.tar.gz
```
> 위 코드를 입력하면 다운이 되는데 안된다면 아래 코드로 wget을 설치해야함(도커 컨테이너에서 실행)

```
apt-get update && apt-get install wget
```
> (도커 컨테이너에서 실행)

3. PostgreSQL Shell 접속 및 데이터 테이블 생성

```
$ psql -U postgres
```
> (도커 컨테이너에서 실행하면 PostgreSQL Shell로 접속하게 됨)

```
create database chembl_30;
```
4. 도커 컨테이너로 빠져나와서 ChEMBL_30 파일 압축해제 및 dmp 파일 데이터 베이스에 저장 (시간 소요)

```bash
$ tar -vxf  chembl_30_postgresql.tar.gz # 압출 풀기
$ cd chembl_30/chemb_30_postgresql
$ pg_restore --no-owner -h localhost -p 5432 -U postgres -d chembl_30 chembl_30_postgresql.dmp
```

5. 생성된 Chembl_30 DB에 접근 및 테이블 생성

```
psql -U postgres chembl_30
```
* Connect to the database, install the cartridge, and create the schema that we’ll use:

```sql
$ create extension if not exists rdkit;
$ create schema rdk;
```
* Create the molecules and build the substructure search index:

```sql
$ select * into rdk.mols from (select molregno,mol_from_ctab(molfile::cstring) m  from compound_structures) tmp where m is not null;
$ create index molidx on rdk.mols using gist(m);
$ alter table rdk.mols add primary key (molregno);
```

* Create some fingerprints and build the similarity search index:

```sql
$ select molregno,torsionbv_fp(m) as torsionbv,morganbv_fp(m) as mfp2,featmorganbv_fp(m) as ffp2 into rdk.fps from rdk.mols;
$ create index fps_ttbv_idx on rdk.fps using gist(torsionbv);
$ create index fps_mfp2_idx on rdk.fps using gist(mfp2);
$ create index fps_ffp2_idx on rdk.fps using gist(ffp2);
$ alter table rdk.fps add primary key (molregno);
```
* Here is a group of the commands used here (and below) in one block so that you can just paste it in at the psql prompt:

```sql
create or replace function get_mfp2_neighbors(smiles text)
returns table(molregno bigint, m mol, similarity double precision) as
$$
select molregno,m,tanimoto_sml(morganbv_fp(mol_from_smiles($1::cstring)),mfp2) as similarity
from rdk.fps join rdk.mols using (molregno)
where morganbv_fp(mol_from_smiles($1::cstring))%mfp2
order by morganbv_fp(mol_from_smiles($1::cstring))<%>mfp2;
$$ language sql stable ;
```

# PostgreSQL & Python
* python버전에 맞는 pip을 다시 설치

```bash
$ apt-get remove python3-pip
$ apt-get install python3-pip
```

## 1. PostgreSQL Docker 컨테이너에 필요 프로그램 설치

```bash
$ apt-get update
$ apt-get install libpq-dev
```
## 2. psycopg2 설치

```
$ pip3 install psycopg2
$ pip3 install psycopg2-binary
```
> conda 가상환경에서 사용

bash: pip: command not found 라는 문구가 나오면 아래 코드 입력

```
$ apt-get install python3-pip
```
> pip가 안 깔려있을때 다운로드할 수 있음.

> pip는 파이썬으로 작성된 패키지 소프트웨어를 설치 · 관리하는 패키지 관리 시스템이다.

> python3버전의 경우 파이썬 명령어도 python3를 사용하는데 pip도 마찬가지로 pip3 명령어를 사용한다.

## 3. pip list 저장
pip freeze > requirements.txt



## 4. 참고한 사이트
["AI로 신약 물질과 타깃 발굴하는 패키지로 승부"_기사](http://www.hitnews.co.kr/news/articleView.html?idxno=35920)

[신약개발에서 딥 러닝의 부상](https://www.ibric.org/myboard/read.php?Board=report&id=3349)

[머신러닝 기반 분자 활성 예측 정확도가 낮은 이유](https://novelism.co.kr/197?category=930849)

[fragment 기반 약물 가상탐색](https://novelism.co.kr/188?category=930849)

[PostgreSQL & RDKit Cartridge & Docker](https://velog.io/@decom0405/PostgreSQL-RDKit-Cartridge-1)

[도커 가상화 리눅스 환경에서 파이썬 버전 변경(Ubuntu 18.04)](https://hobbylists.tistory.com/entry/%EB%8F%84%EC%BB%A4-%EA%B0%80%EC%83%81%ED%99%94-%EB%A6%AC%EB%88%85%EC%8A%A4-%ED%99%98%EA%B2%BD%EC%97%90%EC%84%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC-%EB%B2%84%EC%A0%84-%EB%B3%80%EA%B2%BDUbuntu-1804)

[리눅스 기본명령어](https://www.mireene.com/webimg/linux_tip1.htm)

[Docker compose 사용법1](https://engineer-mole.tistory.com/221)  

[Docker compose 사용법2](https://darrengwon.tistory.com/793)

[Docker compose 를 이용해서 복잡한 도커 컨테이너를 제어하기_유튜브](https://www.youtube.com/watch?v=EK6iYRCIjYs)

[도커를 이용한 파이썬 개발](https://inaj012.medium.com/%EB%8F%84%EC%BB%A4%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-python-%EA%B0%9C%EB%B0%9C-f6f1af58bf6)  

[VS code에 Docker 연동하기](https://89douner.tistory.com/123)

[VS code를 활용하여 원격 서버의 docker container에 접속하는 방법](https://seokhyun2.tistory.com/48)

[Docker image 생성 및 배포하기](https://hororolol.tistory.com/559)

[PostgreSQL & Python](https://velog.io/@decom0405/PostgreSQL-Python)

[SQL Examples](https://chembl.gitbook.io/chembl-interface-documentation/frequently-asked-questions/schema-questions-and-sql-examples#retrieve-all-the-bioactivity-data-for-bacterial-targets)  

[The RDKit database cartridge](https://www.rdkit.org/docs/Cartridge.html)

[파이썬 3.8에서 RDKit 설치](https://mopipe.tistory.com/109)

[SPE2Vec 문서 설명](https://xinhaoli74.github.io/SmilesPE/spe2vec/)

[캐글_Drug Design with Small Molecule SMILES](https://www.kaggle.com/code/art3mis/220221-getting-started-with-smiles)

[코드스테이츠 cp1 피드백](https://codestates.notion.site/AIBootcamp-9-10-Code-States-Project-1-d1a05d1bc6914f91a462963cfba33aae#583f0053f8624a8984e7aa7fa7e559b9)

* 참고논문

[J. Chem. Inf. Model. 2021, 61, 4, 1560–1569](https://pubs.acs.org/doi/10.1021/acs.jcim.0c01127)


## 5. 추가 공부
논문  
[DeepPLA: a novel deep learning-based model for protein-ligand binding affinity
prediction](https://www.biorxiv.org/content/10.1101/2021.12.01.470868v1.full.pdf)  

신약을 시장에 출시하는 것은 위험하고, 시간이 많이 걸리며, 비용이 많이 드는 것이 특징이다.
실제로 2018년에 보고된 바와 같이 신약 개발에는 10~20년, 20억 달러 이상이 소요될 수 있다. 이 단계는 일반적으로 수백 수천 개의 화합물을 포함하지만, 약 8%만이 체외 및 체내 임상 연구 단계에 진입할 수 있다[4].
모든 약물 용도 변경 방법 가운데, 의약품 화합물 라이브러리를 선별하고 약물 표적 상호 작용(DTI)을 식별하는 실리코 계산 기반 방법은 높은 컴퓨팅 아키텍처의 발전과 머신 러닝의 발전 덕분에 점점 더 많은 관심을 얻고 상당한 발전을 이룸.

일반적으로 의약품 연구 및 개발의 전체 프로세스는 (i) 의약품 발견, (ii) 임상 연구, (iii) 임상 시험, (iv) FDA 검토 및 (v) 시판 후 안전성 감시를 포함한 서벌 핵심 단계[3]로 복잡하다. 특히 약물 발견은 단백질과 같은 충족되지 않은 질병의 대상을 식별하는 것으로 시작하여 대상과 효율적이고 안전하게 상호 작용할 수 있는 유망한 화합물을 생성하고 최적화하는 첫 번째 단계이다.

약물 표적 상호 작용(DTI)
목적: 표적 단백질과 약물 화합물 사이의 단백질 리간드 상호 작용의 강도를 측정하는 결합 친화성을 연구하는 것
* 결합 친화성은 일반적으로 inhibition(𝐾𝑖), dissociation(𝐾𝑑) 및 half-maximal inhibitory(𝐼𝐶50)를 나타내는 다음과 같은 상수의 형태로 표현
* 단백질-리간드 상호작용을 식별하려는 기존의 방법은 분자/생물학적 구조의 복잡한 양자 화학 계산을 포함
  * 기존 방법은 긍정적인 약물-표적 쌍을 발견하기 위해 기존 데이터베이스에서 학습할 수 있는 방법이 될 수 없다
  * 지난 10년 동안 랜덤 포레스트(RF) 기반 알고리듬[8][9], SimBoost[10] 및 ChemBoost[11]와 같은 수백만 리간드와 단백질에서 DTI를 식별하기 위해 다양한 기계 학습 기반 모델이 개발
  * 딥 러닝 기반 기법의 출현과 그것의 성공적인 응용은 컴퓨터 비전이나 언어 처리와 같은 응용 프로그램을 넘어 신약을 발견하는 유망한 방법을 개척함
    * 예) 
    * MFDR[12]은 화학 구조 및 단백질 시퀀스에서 자동 인코더로 기능을 얻은 다음 SVM 모델을 사용하여 DTI를 0-1 분류 문제로 예측
    * DeepDTA[13]는 화합물과 단백질의 시퀀스를 정보 모듈로 표현하기 위해 두 개의 다른 CNN 모듈을 별도로 적용
    * 정보 모듈의 결과적 특징은 단백질-리간드 결합 친화성을 예측하기 위해 완전히 연결된 3개의 레이어를 입력
      * Deep에서 간단한 라벨 인코딩 방법의 사용원시 입력 시퀀스(즉, 해당하는 인코딩된 정수를 사용하여 원시 시퀀스에서 기호를 나타냄)를 포함하기 위한 DTA는 원시 시퀀스에 대한 많은 정보를 잃을 수 있다.
      * OnionNet[15]은 단백질과 약물 분자의 3D 구조를 활용하여 약물 표적 결합 친화성[16]을 예측하여 정보 손실 문제를 크게 줄인다. 
      * 불충분한 3D 단백질 구조 데이터[17]로 인해 이러한 모델의 실용성과 정확성은 제한된다. 단백질에 대한 정확한 3D 구조 데이터는 가혹하고 극단적인 조건에서 고급 실험 방법이 필요하기 때문에 얻기 어렵다[18].

* 본 논문에서는 단백질 1D 시퀀스와 1D 리간드 SMILES 문자열을 사용하는 단백질-리간드 결합 친화도 예측을 위한 혁신적인 딥 러닝 기반 모델인 DeepPLA를 제안
  * Simplified Molecular Input Line Entry System의 약자 SMILES는 화학 규칙에 기초한 분자 구조를 표현하기 위한 잘 알려진 콤팩트 선형 표기법이다.
    * Mol2Vec[20] 및 ProSE[21]의 사전 훈련된 모델을 각각 사용하여 리간드 SMILES 문자열과 단백질 시퀀스를 숫자 벡터로 내장한 다음 단백질과 리간드의 내장 숫자 벡터를 Head CNN 모듈과 ResNet 기반 CNN 모듈에 공급하여 각각 단백질과 리간드 시퀀스를 인코딩한다.
    * 인코딩된 표현은 벡터로 연결되고 양방향 LSTM(롱 단기 메모리) 계층에 추가로 공급되며, 이어서 세 개의 완전히 연결된 계층이 이어진다. DeepPLA의 출력은 연속 값이며, 리간드와 단백질의 입력 쌍의 결합 친화성을 나타낸다. 우리는 BindingDB 데이터 세트[22]를 사용하여 DeepPLA 모델을 훈련하고 바인딩 친화도에 대한 성능을 평가한다.
    * 원시 데이터는 𝐾𝑖 값이 없는 단백질-리간드 쌍 제거, 전체 데이터 집합의 80%를 무작위로 선택하여 훈련 집합 구축, 나머지 20%는 평가를 위해 독립적인 시험 집합으로 저장되는 등 먼저 사전 처리된다.
* 본 논문에서는 BindingDB 를 사용했다
  * BindingDB: 2021년 7월 29일까지 실험적으로 확인된 8,005개의 표적 단백질과 986,143개의 작은 약물 분자 사이의 결합 친화성을 포함하는 지속적으로 업데이트되는 데이터 세트
* 모델 개발을 위한 데이터 세트 구성에 적용한 기준
  * (1) excluding binding interactions with multichain protein complexes because it is not capable of identifying which chain of the protein interacts with the molecular; 
  * (2) retaining binding interactions only represented by 𝐾𝑖 value and it means that other measurements in the form of 𝐼𝐶50 or 𝐾𝑑 values are removed; 
  * (3) keeping common drug molecules and target proteins occurring in at least three and six interactions in the entire dataset [11], respectively; 
  * (4) removing data with invalid 𝐾𝑖 values. For example, we notice that some data used “>” and “<” in the 𝐾𝑖 values to indicate ranges and we directly exclude them for the subsequent analysis. Additionally, there are some zeros in the 𝐾𝑖 values which should not appear based on the definition of 𝐾𝑖. Thus, we treat them as invalid values and simply removed them;
  * (5) transferring 𝐾𝑖 values into 𝑝𝐾𝑖 using the following equation (eq.1) due to the skewed distribution of original values of 𝐾𝑖: 𝑝𝐾𝑖 = −log<sub>10</sub> 𝐾𝑖 --- (eq.1)
  * (6) removing data where the 𝑝𝐾𝑖 values are less than -8 because these extremely small 𝑝𝐾𝑖 values suggest that the corresponding drug molecules and protein targets held much weak or even no interactions. As a result, a total of 249,517 interactions with 54,135 drug molecules and 1,844 protein targets are finally used in developing our model.

![](2022-05-18-13-55-41.png)

* Mol2Vec은 분자를 숫자 벡터 표현으로 변환하기 위한 비지도 딥 러닝 기반 접근 방식이다. 자연어 처리(NLP) 기법에 영감을 받아, Mol2Vec은 모건 식별자 [23]에 의해 얻어진 분자 하위 구조를 "단어"로 간주하고, 화합물을 "문장"으로 간주하고, 이를 에 기반한 조밀한 벡터 표현으로 인코딩한다.
이른바 화합물 말뭉치입니다.

* ProSE는 단백질 구조 정보를 인코딩하는 숫자 벡터로 단백질 서열을 표현하기 위해 개발된 딥 러닝 기반 방법이다. 먼저 단백질 서열을 유사한 아미노산(단어)을 가까운 숫자로 매핑하는 특정 알파벳 목록("문장")으로 번역한다. 그런 다음, ProSE 모델은 단어를 숫자 벡터로 인코딩한다. 
* Mol2Vec(다운로드 링크: https://github.com/samoturk/mol2vec)
* ProSE(다운로드 링크, https://github.com/tbepler/prose)
  * 활용하여 각각 약물 분자 화합물과 단백질에 대해 고정된 길이의 벡터 표현을 얻는다.
* PROSE 사용도 사용법에 관한 예시가 없어서 사용하기가 어려움.


## 6. Protein identification을 위한 bio.tools 사이트
MSAC |   
The Molecular Spectrometry Adduct Calculator (MSAC) calculates the m/z of potential adducts from given compound m/z.

RCSB Protein Data Bank |   
Improved Annotation, Search, and Visualization of Membrane Protein Structures Archived in the PDB.

MS2Rescore |   
Sensitive PSM rescoring with predicted MS² peak intensities using MS²PIP, DeepLC, and Percolator.

PeptideShaker Online |   
A User-Friendly Web-Based Framework for the Identification of Mass Spectrometry-Based Proteomics Data.

PROMISed |   
A novel web-based tool to facilitate analysis and visualization of the molecular interaction networks from co-fractionation mass spectrometry (CF-MS) experiments.

PROSE |   
Single-sample proteome enrichment enables missing protein recovery and phenotype association.

RHybridFinder |   
An R package to process immunopeptidomic data for putative hybrid peptide discovery.

VIQoR |   
VIQoR, is a user-friendly web service for Visually supervised protein Inference and protein Quantification implemented in R. The Shiny web interface integrates the post-identification processes involved in protein inference and relative protein abundance summarization, along with smart and novel interactive visualization modules to support the common researchers with a straight-forward tool for protein quantification, data browsing and data inspection.

ProteInfer |   
ProteInfer is an approach for predicting the functional properties of protein sequences using deep neural networks.

prPred-DRLF |   
prPred-DRLF is a tool to identify the plant resistance proteins (R proteins) based on deep representation learning features

## 7. Reference: Learning the protein language: Evolution, structure, and function (Cell Systems 12, 654–669)

* 언어 모델은 최근 대규모 단백질 서열 데이터베이스에서 정보를 증류하기 위한 강력한 기계 학습 접근 방식으로 부상하고 있다.
  * 쉽게 구할 수 있는 시퀀스 데이터만으로, 이러한 모델은 단백질 공간 전반에 걸친 진화적, 구조적, 기능적 조직을 발견한다. 
  * 언어 모델을 사용하여 아미노산 시퀀스를 구조적 및 기능적 특성을 포착하는 분산 벡터 표현으로 인코딩할 수 있을 뿐만 아니라 시퀀스 변형들의 진화적 적합성을 평가할 수 있다. 
  * 본 논문에서는 단백질 언어 모델링의 최근 발전과 다운스트림 단백질 특성 예측 문제에 대한 응용에 대해 논의한다. 그런 다음 이러한 모델이 사전 생물학적 지식으로 풍부해질 수 있는 방법을 고려하고 단백질 구조 지식을 학습된 표현으로 인코딩하기 위한 접근 방식을 소개한다. 
  * 이러한 모델에 의해 증류된 지식은 전이 학습을 통해 다운스트림 함수 예측을 개선할 수 있게 한다. 
  * 심층 단백질 언어 모델은 단백질 생물학에 혁명을 일으키고 있다. 그들은 단백질과 치료 설계에 접근하는 새로운 방법을 제안한다. 
    * 그러나 강력한 생물학적 사전들을 단백질 언어 모델로 인코딩하고 더 넓은 커뮤니티에 대한 접근성을 높이기 위해서는 추가적인 개발이 필요하다.


  * 단백질은 세포의 분자 기능의 대부분을 수행하는 분자 기계이다.
    * 이들은 3차원 구조의 복잡한 앙상블에 접히는 아미노산의 선형 배열로 구성되며, 순서에서 무질서한 상태까지 다양하며 입체구조 변화를 겪을 수 있다.
    * 생화학적, 세포적 기능은 단백질 서열과 구조에서 나타난다. 서열-구조-기능 관계를 이해하는 것은 단백질 생물학의 핵심 문제이며 질병 메커니즘을 이해하고 치료 및 생명공학 응용을 위한 단백질과 약물을 설계하는 데 중추적이다. 
    * 시퀀스-구조-함수 관계의 복잡성은 부분적으로 기존 도구가 대규모 데이터베이스에 저장된 시퀀스, 구조 및 기능 정보의 증가하는 양의 잠재력을 완전히 실현하지 못하기 때문에 우리의 계산 모델링 능력에 계속 도전한다.
    * 최근까지 단백질 분석을 위한 계산 방법은 진화적, 따라서 기능적 압력을 반영하는 시퀀스 패턴을 식별하려는 최초의 원리 기반 구조 시뮬레이션 또는 통계적 시퀀스 모델링 접근법을 사용해 왔다(Marks, Hopf and Sander, 2012; Ekeberg et al., 2013; Wang et al., 2017; Liu et). al. , 2018; Yang et al., 2020) (그림 1) 이러한 방법들 내에서 구조 분석은 주로 첫 번째 원칙인 반면, 시퀀스 분석 방법은 주로 진화 과정에 대한 강력한 가정을 하는 통계 시퀀스 모델을 기반으로 하지만 이용 가능한 자연 시퀀스 정보의 양이 증가함에 따라 점점 더 데이터 중심화되었다.
    * 물리 기반 접근법은 모든 원자 에너지 함수(Hornak et al., 2006; Hess et al., 2008; Alford et al., 2017) 또는 휴리스틱을 사용한다.
      * 주어진 구성의 에너지를 추정하고 자연적인 움직임을 시뮬레이션하기 위해 단백질을 위해 설계되었다(Rohlet et al., 2004). 이 방법들은 매력적인데, 왜냐하면 이 방법들은 이 시스템들의 물리학에 대한 우리의 근본적인 이해를 끌어내고 해석 가능한 가설을 생성하기 때문이다. 
        * 작은 일정한 크기의 연속 시퀀스와 관련된 접힌 파편을 꿰맨 Rosetta 도구는 단백질 접힘 및 설계를 위한 자유 에너지 추정(Leaver-Fay et al., 2011)을 사용하는 데 현저하게 성공했으며, GROMACS와 같은 분자 역학 소프트웨어는 역학 모델링과 구조 예측 세분화(Hess et al., 2008)에 널리 사용된다. 
        * 거친 입자 에너지 함수를 기반으로 접근 가능한 구성에서 표본을 추출하는 통계 샘플링 접근 방식도 개발되었다(Godzik, Kolinski 및 Scolnick, 1993; Srinivasan and Rose, 1995; Choi and Pappu, 2019). Rosetta는 특히 구조 템플릿과 자유 에너지 최소화를 혼합하여 대상 구조와 일치하는 시퀀스를 찾아 설계 문제를 해결하는 데 성공했다. 그러나 Rosetta의 성공에도 불구하고, Rosetta와 유사한 접근 방식은 단순화된 에너지 모델을 가정하고, 계산 비용이 매우 비싸며, 올바르게 설정하기 위해 전문 지식이 필요하며, 정확도가 제한적이다.

알파폴드2 공부해볼 것!
  * 단백질의 분산 벡터 표현이 단백질 시퀀스의 생성 모델에서 추출될 수 있고, 자연 단백질 공간에 걸쳐 크고 다양한 시퀀스의 데이터베이스로부터 학습될 수 있으며, 따라서 주어진 시퀀스의 의미 또는 기능을 포착할 수 있다는 생각에 기초
    * 기능은 단백질이 하는 것과 관련된 모든 성질
  * 자기 회귀 언어 모델에서, 시퀀스의 확률은 각 토큰의 확률이 이전 토큰에서만 조건화되도록 인수 분해된다.
    * 이 인수 분해는 정확하며 분포에서 표본을 추출하거나 확률 자체를 평가하는 데 유용
    * 공식의 단점은 각 위치에 대해 학습된 표현이 이전 위치에만 의존하여 잠재적으로 상황별 표현으로 덜 유용
  * 구글의 BERT와 오픈AI의 GTP-3는 거대한 온라인 말뭉치의 수십억 개의 텍스트 항목에서 학습하는 능력에 의해 주로 구동
  * 최근 NLP의 발전 이유
    * neural network architectures, 
    * new training approaches, 
    * increasing compute power, and 
    * increasing accessibility of huge text corpuses. 
  * LSTMs are recurrent neural networks
    * 한 번에 하나의 토큰 시퀀스를 순서대로 처리하므로 위치와 모든 이전 위치에서 정보를 캡처하는 표현을 학습
    * 주어진 위치 전후에 토큰의 정보를 포함하기 위해, 양방향 LSTM은 각 레이어의 순방향과 역방향으로 작동하는 두 개의 개별 LSTM을 결합
    * 전체 시퀀스 컨텍스트를 포함한 표현을 학습할 수 있지만, 실제로 원격 종속성을 학습하는 능력은 제한적
    * 한계를 해결하기 위해, 변압기는 시퀀스의 각 위치에 대한 주의 벡터를 명시적으로 계산하여 표현을 학습
  * self-attention module 
    * 시퀀스의 각 요소의 출력 표현은 가중치 자체가 입력의 학습된 변환을 기반으로 하는 각 위치에서 입력 표현의 변환에 대한 가중치 합으로 계산됨
  * attention mechanism 
    * 일반적으로 변압기가 선형 시퀀스에서 멀리 있는 위치 간의 종속성을 더 쉽게 학습할 수 있도록 함
  * Transformers
    * autoregressive language models로도 유용
    * 자연어 처리에서 피터스 등은 biLSTM의 숨겨진 계층(스택 신경망의 중간 표현)이 문맥에서 단어의 의미적 의미를 인코딩한다는 것을 인식했다. 이 관찰은 생물학적 서열 분석을 위해 새롭게 활용
    * Although transformers are powerful models, they require enormous numbers of parameters and train more slowly than typical recurrent neural networks. 
  * 결론 및 관점
  * 단백질 언어 모델을 개선하는 열쇠 단백질 언어 모델링 및 표현 학습의 향후 발전은 단백질에 고유한 특성을 모델링해야 할 것
  * 생물학적 서열은 자연 언어가 아니며, 우리는 생물학적 서열의 근본적인 본질을 포착하는 새로운 언어 모델을 개발해야 함

## 8. Reference: Protein embeddings and deep learning predict binding residues for various ligand classes (Scientifc Reports | (2021) 11:23916)

* 단백질 기능의 한 가지 중요한 측면은 작은 분자, 금속 이온, DNA 또는 RNA와 같은 고분자를 포함하는 리간드에 단백질의 결합이다. 
* 본 논문은 단백질 잔기가 금속 이온, 핵산 또는 작은 분자에 결합하는지 예측하는 방법인 bindEmbed21을 제안
* 인공지능(AI) 기반 방법은 트랜스포머 기반 단백질 언어 모델(pLM) ProtT5의 임베딩을 입력으로 독점적으로 사용
* 다중 시퀀스 정렬(MSA)을 생성하지 않고 단일 시퀀스만 사용하여 bindEmbed21DL은 MSA 기반 예측을 능가함
* 가장 강하게 예측된 25% 결합 잔기의 경우, 실험 주석 누락 문제를 무시하더라도 적어도 73%가 정확하게 예측됨
* 구조나 MSA를 사용하지 않고 빠르고 단순하며 광범위하게 적용가능

* 축약어
  * AI Artifcial intelligence (expanding ML through deep learning, i.e., using more free parameters)
  * BFD Big Fantastic Database (large database of protein sequences)
  * CI Confdence interval
  * CNN Convolutional Neural Network
  * HBI Homology-based inference
  * (p)LM (Protein) language model
  * MCC Matthews Correlation Coefcient
  * ML Machine learning
  * MSA Multiple sequence alignment
  * PDB Protein Data Bank
  * PIDE Pairwise sequence identity
  * SOTA State-of-the-art
  * SVM Support vector machine

* AlphaFold 2는 다중 시퀀스 정렬(MSA)의 정보에 크게 의존한다. MSA가 없는 최근의 구조 예측은 덜 정확하다. 구조 예측이 결합 잔류물에서 결합 부위로의 단계를 넘어 결합 예측을 어느 정도까지 개선할 수 있을지는 여전히 불분명하다.
* bindEmbed21: 

## 9. Reference: Biological structure and function emerge from scalingunsupervised learning to 250 million protein sequences(PNAS2021 Vol. 118 No. 15 e2016239118)

* 단백질 서열은 자연어와 매우 다른 과정으로부터 발생한다. 모델과 목적 함수가 도메인 간의 차이에 걸쳐 자연어 전달에 효과적인지는 불확실하다. 우리는 진화 데이터에 대한 고용량 트랜스포머 언어 모델을 훈련함으로써 이 질문을 탐구한다. 우리는 생물학적 조직 원리와 본질적인 생물학적 특성에 대한 정보의 존재에 대한 결과적인 비지도 표현을 조사한다. 우리는 물리화학적 규모에서 원격 상동학에 이르기까지 조직 원리에 부합하는 메트릭 구조를 표현 공간에서 발견한다. 우리는 또한 표현에서 2차 및 3차 단백질 구조가 식별될 수 있다는 것을 발견했다. 표현에 의해 캡처된 구조적 특성은 여러 폴더에 걸쳐 일반화된다. 우리는 표현을 다양한 예측 작업에 적용하고 응용 프로그램 전체에서 최첨단 기능을 향상시킨다는 것을 발견했다.

* 시퀀스 데이터에서 생물학적 특성을 배우는 것은 생물학을 위한 생성 및 예측 인공지능을 향한 논리적 단계이다. 여기서는 비지도 학습을 통해 심층 문맥 언어 모델을 진화적 다양성에 걸친 시퀀스로 확장할 것을 제안한다. 우리는 사전 지식 없이 2차 구조, 접촉 및 생물학적 활동과 같은 단백질의 기본 특성에 대한 학습된 표현에서 정보가 나타난다는 것을 발견했다. 우리는 학습된 표현이 원격 호몰로지 감지, 2차 구조 예측, 장거리 잔류-잔류 접촉 및 돌연변이 효과를 위한 벤치마크에서 유용하다는 것을 보여준다. 비지도 표현 학습은 돌연변이 효과와 2차 구조에 대한 최첨단 지도 예측을 가능하게 하고 장거리 접촉 예측을 위한 최첨단 기능을 향상시킨다.
* Recently, self-supervision(자기지도학습) has emerged as a core direction inartificial intelligence research.
  * 본 논문은 다양한 자연어 처리 작업에서 최첨단 성능을 입증한 자체 지도 언어 모델링 접근 방식을 탐색하여 레이블이 지정되지 않은 아미노산 시퀀스의 형태로 단백질 데이터에 적용
  * 단백질 시퀀스는 20개의 표준 요소의 작은 어휘를 사용하기 때문에, 모델링 문제는 단어 수준 모델보다 문자 수준 언어 모델(30, 31)과 더 유사
  * 자연어처럼 단백질 서열도 장거리 의존성을 포함하며, 원거리 컨텍스트를 감지하고 모델링하는 아키텍처의 사용을 동기 부여함
* We investigate the Transformer (32), which has emerged as a powerful general purpose model architecture for representation learning and generative modeling, outperforming recurrent and convolutional architectures in natural language settings. 

[ESM](https://github.com/facebookresearch/esm)  
[ESM Embedding 설명](https://github.com/facebookresearch/esm/blob/main/examples/sup_variant_prediction.ipynb)

```
& python3 scripts/extract.py esm1b_t33_650M_UR50S examples/data/some_proteins.fasta examples/data/some_proteins_emb_esm1b/ \
    --repr_layers 0 32 33 --include mean per_tok
```
```python
scripts/extract.py
# extract.py 파일의 코드 사용됨
# scripts/extract.py has flags that determine what's included in the .pt file

esm1b_t33_650M_UR50S
# 사전에 학습된 자료? 모델?

examples/data/some_proteins.fasta
# fasta file (sequence 입력된 파일)

examples/data/some_proteins_emb_esm1b/ 파일생성
# 코드가 실행되면 examples/data/some_proteins_emb_esm1b/ 파일에 결과 파일 생성
# 디렉토리 some_directory_filename_esm1b/에 FASTA 시퀀스당 하나의 .pt 파일이 포함되어 있습니다. torch.load to를 사용하여 파일을 로드하면 됨.

```

* FASTA format은 생물 정보학 및 생화학에서 nucleotide sequence 또는 amino acid (protein) sequence를 나타내는 text-based format으로 nucleotide 또는 amino acids는 단일 문자 코드 (one-letter-code)를 사용하여 표현

* 예시

```
>gi|5524211|gb|AAD44166.1| cytochrome b [Elephas maximus maximu]
LCLYTHIGRNIYYGSYLYSETWNTGIMLLLITMATAFMGYVLPWGQMSFWGATVITNLFSAIPYIGTNLV
EWIWGGFSVDKATLNRFFAFHFILPFTMVALAGVHLTFLHETGSNNPLGLTSDSDKIPFHPYYTIKDFLG
LLILILLLLLLALLSPDMLGDPDNHMPADPLNTPLHIKPEWYFLFAYAILRSVPNKLGGVLALFLSIVIL
GLMPFLHTSKHRSMMLRPLSQALFWTLTMDLLTLTWIGSQPVEYPYTIIGQMASILYFSIILAFLPIAGX
IENY
```
* NCBI는 생명과학 및 의학 논문 인덱스의 데이터베이스인 펍메드(PubMed), 유전체 서열 데이터베이스인 진뱅크(GenBank)를 비롯하여 각종 생명공학 정보들을 담고 있으며, 이 모든 정보들은 Entrez 검색엔진을 이용하여 온라인으로 열람할 수 있다.

* ~.pt 는 pytorch 모델 파일의 확장자이다.
* ESM 관련 논문은 나중에 다시 구현해보기.

## 10. 깃허브로 도커 이미지 올리기

Github.com Packages Container Registry

깃허브 패키지에서 도커는 안쓰이고 컨테이너 사용하면 된다.
![](2022-05-30-09-52-57.png)

1. PAT(Personal access tokens) 발급받기
   명령어를 이용해서 패키지를 제어할수 있도록  
   write-packages  
   delete-packages  
   두가지를 on 시키고 발급받은 토큰은 저장해야함.

   
2. 도커를 깃허브로 로그인
   docker login ghcr.io -u username -p ~~ 
   
   ghcr.io : 깃허브 레지스트리 주소를 입력  
   -u 뒤는 ID(github username)이다. (ID: egoing)
   -p 뒤 : 토큰 입력
   토큰까지 입력하면 히스토리에 남기때문에 다른사람도 로그인 가능성이 있기때문

   그래서 export CR_PAT 사용 &rArr; 환경변수에 토큰 저장

```bach
export CR_PAT = ~~(PAT 입력)
echo $CR_PAT
echo $CR_PAT | docker login ghcr.io -u username --password-stdin
```
--password-stdin 는 CR_PAT 값이다.  
실행마다 해야하는 불편함을 없애려면  
shell이 실행될때마다 사용할 수 있도록 설정하면된다.

3. 도커 컨테이너를 이미지로 저장하는 방법

컨테이너 변경 사항 확인

```
docker diff <CONTAINER_ID>
```
컨테이너 저장

```
docker commit <CONTAINER_ID> <IMAGE_NAME>:<TAG>
 ```

Docker Hub에 이미지 Push

```
# 레지스트리에 로그인
docker login

# push
docker push <IMAGE_NAME>:<TAG>
```

github에 이미지 Push방법

```
docker commit db_dpcr ghcr.io/choidb/cproject_db:1.0
docker push ghcr.io/choidb/cproject_db:1.0
```


